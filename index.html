<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper API Audio Transcription Tool</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            text-align: center;
            color: #333;
        }
        .container {
            background-color: #f9f9f9;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .upload-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            margin-bottom: 20px;
        }
        #upload-btn {
            background-color: #4CAF50;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        #upload-btn:hover {
            background-color: #45a049;
        }
        #file-input {
            display: none;
        }
        #transcription-status {
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }
        #transcription-result {
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            margin-top: 20px;
            min-height: 100px;
            max-height: 500px;
            overflow-y: auto;
            white-space: pre-wrap;
            font-family: monospace;
        }
        .audio-player {
            width: 100%;
            margin-top: 10px;
        }
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(0,0,0,.3);
            border-radius: 50%;
            border-top-color: #4CAF50;
            animation: spin 1s ease-in-out infinite;
        }
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        .api-section {
            margin-top: 20px;
            padding: 15px;
            background-color: #fff8e1;
            border-radius: 8px;
            border: 1px solid #ffe082;
        }
        .api-input {
            width: 100%;
            padding: 8px;
            margin-top: 5px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .progress-container {
            width: 100%;
            background-color: #f1f1f1;
            border-radius: 4px;
            margin-top: 15px;
            position: relative;
            height: 25px;
            overflow: hidden;
        }
        .progress-bar {
            height: 100%;
            background-color: #4CAF50;
            width: 0%;
            transition: width 0.3s ease;
            border-radius: 4px;
        }
        .progress-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: #333;
            font-weight: bold;
            text-shadow: 1px 1px 1px rgba(255,255,255,0.5);
        }
        .options-section {
            margin-top: 15px;
            padding: 10px;
            background-color: #e3f2fd;
            border-radius: 4px;
        }
        .options-toggle {
            font-weight: bold;
            cursor: pointer;
            user-select: none;
            display: flex;
            align-items: center;
        }
        .options-toggle:hover {
            color: #1565c0;
        }
        .options-toggle::after {
            content: "▼";
            margin-left: 5px;
            font-size: 12px;
        }
        .options-toggle.collapsed::after {
            content: "►";
        }
        .options-content {
            margin-top: 10px;
            padding: 10px;
            background-color: #fff;
            border-radius: 4px;
        }
        .hidden {
            display: none;
        }
        .chunk-status {
            margin-top: 5px;
            font-size: 14px;
            color: #666;
        }
    </style>
</head>
<body>
    <h1>Whisper API Audio Transcription Tool</h1>
    
    <div class="container">
        <div class="upload-section">
            <label for="file-input" id="upload-btn">Choose Audio File</label>
            <input type="file" id="file-input" accept="audio/*">
            <p id="file-name">No file selected</p>
        </div>
        
        <div id="audio-container" style="display: none;">
            <audio id="audio-player" controls class="audio-player"></audio>
        </div>
        
        <div class="api-section">
            <p><strong>OpenAI Whisper API Integration</strong></p>
            <p>Enter your OpenAI API key to use the Whisper transcription service:</p>
            <input type="password" id="api-key" class="api-input" placeholder="Enter your OpenAI API key here">
            <p style="font-size: 12px; color: #777;">Your API key is only sent directly to OpenAI and is not stored.</p>
            <p style="margin-top: 10px; font-size: 14px; color: #ff6d00;">
                <strong>Note about Speaker Diarization:</strong> OpenAI's Whisper API doesn't currently support speaker identification. 
                For diarization, you'll need to use services like AssemblyAI or Rev.ai after obtaining the transcript.
            </p>
            <button id="transcribe-btn" style="margin-top: 10px; padding: 8px 16px; background-color: #2196F3; color: white; border: none; border-radius: 4px; cursor: pointer;">Transcribe</button>
        </div>
        
        <div class="options-section">
            <div class="options-toggle" id="options-toggle">Advanced Options</div>
            <div class="options-content hidden" id="options-content">
                <div>
                    <label>
                        <input type="checkbox" id="chunk-audio" checked>
                        Split long audio into chunks (recommended for files >10 minutes)
                    </label>
                </div>
                <div style="margin-top: 10px;">
                    <label>Chunk size (minutes): 
                        <input type="number" id="chunk-size" value="10" min="1" max="30" style="width: 60px">
                    </label>
                </div>
                <div style="margin-top: 10px;">
                    <label>Response format: 
                        <select id="response-format" style="padding: 5px; border-radius: 4px; border: 1px solid #ddd;">
                            <option value="text">Plain text</option>
                            <option value="vtt">VTT (with timestamps)</option>
                            <option value="srt">SRT (with timestamps)</option>
                            <option value="verbose_json">JSON (with detailed timestamps)</option>
                        </select>
                    </label>
                </div>
                <div style="margin-top: 10px;">
                    <label>
                        <input type="checkbox" id="stream-results" checked>
                        Stream results as chunks complete
                    </label>
                </div>
            </div>
        </div>

        <div id="transcription-status"></div>
        
        <div class="progress-container hidden" id="progress-container">
            <div class="progress-bar" id="progress-bar"></div>
            <div class="progress-text" id="progress-text">0%</div>
        </div>
        
        <div id="chunk-status" class="chunk-status"></div>
        
        <pre id="transcription-result"></pre>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // DOM elements
            const fileInput = document.getElementById('file-input');
            const fileName = document.getElementById('file-name');
            const uploadBtn = document.getElementById('upload-btn');
            const audioPlayer = document.getElementById('audio-player');
            const audioContainer = document.getElementById('audio-container');
            const transcriptionStatus = document.getElementById('transcription-status');
            const transcriptionResult = document.getElementById('transcription-result');
            const transcribeBtn = document.getElementById('transcribe-btn');
            const optionsToggle = document.getElementById('options-toggle');
            const optionsContent = document.getElementById('options-content');
            const progressContainer = document.getElementById('progress-container');
            const progressBar = document.getElementById('progress-bar');
            const progressText = document.getElementById('progress-text');
            const chunkStatus = document.getElementById('chunk-status');
            
            let audioFile = null;
            let isProcessing = false;
            
            // Toggle advanced options
            optionsToggle.addEventListener('click', function() {
                optionsContent.classList.toggle('hidden');
                optionsToggle.classList.toggle('collapsed');
            });
            
            // Handle file selection
            fileInput.addEventListener('change', function(e) {
                audioFile = e.target.files[0];
                if (audioFile) {
                    fileName.textContent = audioFile.name;
                    
                    // Create audio URL and show player
                    const audioURL = URL.createObjectURL(audioFile);
                    audioPlayer.src = audioURL;
                    audioContainer.style.display = 'block';
                    
                    transcriptionStatus.textContent = '';
                    transcriptionResult.textContent = '';
                    progressContainer.classList.add('hidden');
                } else {
                    fileName.textContent = 'No file selected';
                    audioContainer.style.display = 'none';
                }
            });
            
            // Prevent multiple dialogs
            uploadBtn.addEventListener('click', function() {
                fileInput.click();
            });
            
            // Format seconds to HH:MM:SS.mmm
            function formatTime(seconds) {
                const hours = Math.floor(seconds / 3600);
                const minutes = Math.floor((seconds % 3600) / 60);
                const secs = Math.floor(seconds % 60);
                const ms = Math.floor((seconds % 1) * 1000);
                
                return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(3, '0')}`;
            }
            
            // Get estimated audio duration from file size (rough estimate)
            function getEstimatedDuration(file) {
                // Very rough approximation: 1MB ≈ 1 minute for compressed audio
                const fileSizeInMB = file.size / (1024 * 1024);
                return fileSizeInMB; // Returns approximate minutes
            }
            
            // Split a Blob into chunks of the specified size
            function splitAudioIntoChunks(file, chunkSizeInMB = 25) {
                return new Promise((resolve) => {
                    const fileSize = file.size;
                    const chunks = [];
                    const chunkSize = chunkSizeInMB * 1024 * 1024; // Convert MB to bytes
                    const totalChunks = Math.ceil(fileSize / chunkSize);
                    
                    for (let i = 0; i < totalChunks; i++) {
                        const start = i * chunkSize;
                        const end = Math.min(start + chunkSize, fileSize);
                        const chunk = file.slice(start, end);
                        
                        // Create a new File object with the same type as the original
                        const chunkFile = new File([chunk], `chunk_${i+1}_${file.name}`, { type: file.type });
                        chunks.push(chunkFile);
                    }
                    
                    resolve(chunks);
                });
            }
            
            // Transcribe with OpenAI Whisper API
            async function transcribeAudio(file) {
                const apiKey = document.getElementById('api-key').value.trim();
                const responseFormat = document.getElementById('response-format').value;
                
                if (!apiKey) {
                    throw new Error('Please enter your OpenAI API key');
                }
                
                // Check file type
                const fileExt = file.name.split('.').pop().toLowerCase();
                const supportedTypes = ['mp3', 'wav', 'ogg', 'm4a', 'flac', 'webm'];
                
                if (!supportedTypes.includes(fileExt)) {
                    throw new Error('Unsupported file format. Please use MP3, WAV, OGG, M4A, FLAC, or WEBM.');
                }
                
                // Create form data for the API request
                const formData = new FormData();
                formData.append('file', file);
                formData.append('model', 'whisper-1');
                formData.append('response_format', responseFormat);
                
                try {
                    // Call OpenAI Whisper API
                    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${apiKey}`
                        },
                        body: formData
                    });
                    
                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(errorData.error?.message || 'API request failed');
                    }
                    
                    // Handle different response formats
                    if (responseFormat === 'json' || responseFormat === 'verbose_json') {
                        const data = await response.json();
                        return {
                            text: data.text || '',
                            format: responseFormat,
                            rawData: data
                        };
                    } else {
                        const text = await response.text();
                        return {
                            text: responseFormat === 'text' ? text : '',
                            format: responseFormat,
                            rawData: text
                        };
                    }
                } catch (error) {
                    console.error('Transcription error:', error);
                    throw new Error(error.message || 'Failed to transcribe audio');
                }
            }
            
            // Format transcription results based on format
            function formatTranscriptionResult(result, format) {
                if (!result) return "No transcription data returned";
                
                // Handle string results (backward compatibility)
                if (typeof result === 'string') return result;
                
                try {
                    // Handle different formats
                    if (format === 'text') {
                        return result.text || "No text content returned";
                    } else if (format === 'verbose_json') {
                        if (result.rawData && result.rawData.segments && result.rawData.segments.length > 0) {
                            return result.rawData.segments.map(segment => {
                                const startTime = formatTime(segment.start);
                                const endTime = formatTime(segment.end);
                                return `[${startTime} → ${endTime}] ${segment.text}`;
                            }).join('\n\n');
                        } else {
                            return result.text || "No segments found in JSON response";
                        }
                    } else if (format === 'vtt' || format === 'srt') {
                        // VTT/SRT formats are returned as raw text
                        return result.rawData || result.text || "No formatted text returned";
                    } else {
                        // Fallback to whatever we have
                        return result.text || JSON.stringify(result, null, 2) || "Unrecognized format";
                    }
                } catch (error) {
                    console.error("Error formatting transcription result:", error);
                    // Fallback to any text we have
                    if (result.text) return result.text;
                    if (result.rawData) {
                        if (typeof result.rawData === 'string') return result.rawData;
                        return JSON.stringify(result.rawData, null, 2);
                    }
                    return "Error formatting transcription result";
                }
            }
            
            // Adjust timestamps in SRT/VTT formats for chunks
            function adjustTimestamps(text, format, timeOffsetSeconds, existingText = '') {
                if (!text) return '';
                
                // For SRT: adjust cue numbers and timestamps
                if (format === 'srt') {
                    const lastCueNumber = getLastCueNumber(existingText);
                    const lines = text.split('\n');
                    let adjusted = '';
                    let cueNumber = lastCueNumber;
                    
                    for (let i = 0; i < lines.length; i++) {
                        const line = lines[i];
                        
                        // If line is a number (cue number)
                        if (/^\d+$/.test(line.trim())) {
                            cueNumber++;
                            adjusted += cueNumber + '\n';
                        } 
                        // If line is a timestamp
                        else if (line.includes('-->')) {
                            const adjustedTimestamp = adjustTimestampLine(line, timeOffsetSeconds);
                            adjusted += adjustedTimestamp + '\n';
                        }
                        // Otherwise, just add the line
                        else {
                            adjusted += line + '\n';
                        }
                    }
                    
                    return adjusted;
                }
                
                // For VTT: adjust timestamps
                if (format === 'vtt') {
                    const lines = text.split('\n');
                    let adjusted = '';
                    let skipHeader = true; // Skip WEBVTT header for appended chunks
                    
                    for (let i = 0; i < lines.length; i++) {
                        const line = lines[i];
                        
                        // Skip the WEBVTT header line for chunks after the first
                        if (skipHeader && line.includes('WEBVTT')) {
                            skipHeader = false;
                            continue;
                        }
                        
                        // If line is a timestamp
                        if (line.includes('-->')) {
                            const adjustedTimestamp = adjustTimestampLine(line, timeOffsetSeconds);
                            adjusted += adjustedTimestamp + '\n';
                        }
                        // Otherwise, just add the line
                        else {
                            adjusted += line + '\n';
                        }
                    }
                    
                    return adjusted;
                }
                
                return text;
            }
            
            // Get the last cue number from existing SRT text
            function getLastCueNumber(text) {
                if (!text) return 0;
                
                const matches = text.match(/^(\d+)$/gm);
                if (matches && matches.length > 0) {
                    return parseInt(matches[matches.length - 1], 10);
                }
                
                return 0;
            }
            
            // Adjust a single timestamp line
            function adjustTimestampLine(line, offsetSeconds) {
                // Extract timestamps
                const timestampRegex = /(\d{2}):(\d{2}):(\d{2})[\.,](\d{3}) --> (\d{2}):(\d{2}):(\d{2})[\.,](\d{3})/;
                return line.replace(timestampRegex, (match, h1, m1, s1, ms1, h2, m2, s2, ms2) => {
                    // Convert to seconds
                    let startTime = parseInt(h1) * 3600 + parseInt(m1) * 60 + parseInt(s1) + parseInt(ms1) / 1000;
                    let endTime = parseInt(h2) * 3600 + parseInt(m2) * 60 + parseInt(s2) + parseInt(ms2) / 1000;
                    
                    // Add offset
                    startTime += offsetSeconds;
                    endTime += offsetSeconds;
                    
                    // Convert back to timestamp format
                    return `${formatTime(startTime)} --> ${formatTime(endTime)}`;
                });
            }
            
            // Process file in chunks if needed
            async function processAudioFile(file, updateProgress, updateChunkStatus, updateResult) {
                const useChunking = document.getElementById('chunk-audio').checked;
                const streamResults = document.getElementById('stream-results').checked;
                const estimatedDuration = getEstimatedDuration(file);
                const chunkSizeInMinutes = parseInt(document.getElementById('chunk-size').value) || 10;
                const chunkSizeInMB = chunkSizeInMinutes; // Rough approximation: 1 minute ≈ 1MB
                const responseFormat = document.getElementById('response-format').value;
                
                // If file is small or chunking is disabled, process directly
                if (!useChunking || file.size <= 25 * 1024 * 1024) {
                    updateProgress(0);
                    const result = await transcribeAudio(file);
                    updateProgress(100);
                    return result;
                }
                
                // Split file into chunks
                updateChunkStatus('Splitting audio file into chunks...');
                const chunks = await splitAudioIntoChunks(file, chunkSizeInMB);
                
                // Process each chunk
                let combinedResult = {
                    text: '',
                    format: responseFormat,
                    rawData: responseFormat === 'verbose_json' ? { segments: [] } : ''
                };
                
                for (let i = 0; i < chunks.length; i++) {
                    const chunk = chunks[i];
                    const progress = Math.round((i / chunks.length) * 100);
                    updateProgress(progress);
                    updateChunkStatus(`Processing chunk ${i+1} of ${chunks.length}...`);
                    
                    try {
                        const chunkResult = await transcribeAudio(chunk);
                        
                        // Handle different response formats for combining results
                        if (responseFormat === 'verbose_json') {
                            if (chunkResult.rawData && chunkResult.rawData.segments) {
                                // Adjust timestamps for chunks after the first
                                if (i > 0) {
                                    const timeOffset = i * chunkSizeInMinutes * 60; // offset in seconds
                                    chunkResult.rawData.segments.forEach(segment => {
                                        segment.start += timeOffset;
                                        segment.end += timeOffset;
                                    });
                                }
                                
                                combinedResult.rawData.segments = [
                                    ...combinedResult.rawData.segments,
                                    ...chunkResult.rawData.segments
                                ];
                            }
                            combinedResult.text += chunkResult.text + ' ';
                        } else if (responseFormat === 'srt' || responseFormat === 'vtt') {
                            // For SRT/VTT we need to adjust the timestamps and cue numbers
                            if (i > 0) {
                                const timeOffset = i * chunkSizeInMinutes * 60; // offset in seconds
                                const adjustedText = adjustTimestamps(chunkResult.rawData, responseFormat, timeOffset, combinedResult.rawData);
                                combinedResult.rawData += '\n' + adjustedText;
                            } else {
                                combinedResult.rawData = chunkResult.rawData;
                            }
                            combinedResult.text += chunkResult.text + ' ';
                        } else {
                            // Plain text
                            combinedResult.text += chunkResult.text + ' ';
                            combinedResult.rawData += chunkResult.rawData + ' ';
                        }
                        
                        // Update the result if streaming is enabled
                        if (streamResults) {
                            const formattedResult = formatTranscriptionResult(combinedResult, responseFormat);
                            updateResult(formattedResult);
                        }
                    } catch (error) {
                        throw new Error(`Error processing chunk ${i+1}: ${error.message}`);
                    }
                }
                
                updateProgress(100);
                updateChunkStatus('All chunks processed successfully!');
                
                // Trim any extra spaces
                combinedResult.text = combinedResult.text.trim();
                if (typeof combinedResult.rawData === 'string') {
                    combinedResult.rawData = combinedResult.rawData.trim();
                }
                
                return combinedResult;
            }
            
            // Start transcription process
            transcribeBtn.addEventListener('click', async function() {
                if (isProcessing) {
                    alert('Transcription is already in progress. Please wait.');
                    return;
                }
                
                if (!audioFile) {
                    transcriptionStatus.textContent = 'Please select an audio file first';
                    return;
                }
                
                isProcessing = true;
                
                // Show loading indicator and progress bar
                transcriptionStatus.innerHTML = 'Preparing transcription... <span class="loading"></span>';
                transcriptionResult.textContent = '';
                
                progressContainer.classList.remove('hidden');
                progressBar.style.width = '0%';
                progressText.textContent = '0%';
                chunkStatus.textContent = '';
                
                // Function to update progress bar
                const updateProgress = (percent) => {
                    progressBar.style.width = `${percent}%`;
                    progressText.textContent = `${percent}%`;
                };
                
                // Function to update chunk status
                const updateChunkStatus = (message) => {
                    chunkStatus.textContent = message;
                };
                
                // Function to update transcription result
                const updateResult = (text) => {
                    if (text) {
                        transcriptionResult.textContent = text;
                    }
                };
                
                try {
                    // Get file size in MB
                    const fileSizeMB = audioFile.size / (1024 * 1024);
                    
                    if (fileSizeMB > 25 && !document.getElementById('chunk-audio').checked) {
                        throw new Error('File size exceeds 25MB. Please enable audio chunking in Advanced Options.');
                    }
                    
                    // Process audio file (with chunking if needed)
                    transcriptionStatus.innerHTML = 'Transcribing... <span class="loading"></span>';
                    const result = await processAudioFile(audioFile, updateProgress, updateChunkStatus, updateResult);
                    
                    transcriptionStatus.textContent = 'Transcription complete!';
                    
                    // Format and display the final result
                    const responseFormat = document.getElementById('response-format').value;
                    const formattedResult = formatTranscriptionResult(result, responseFormat);
                    transcriptionResult.textContent = formattedResult;
                } catch (error) {
                    transcriptionStatus.textContent = 'Error: ' + error.message;
                    console.error('Transcription error:', error);
                    progressContainer.classList.add('hidden');
                } finally {
                    isProcessing = false;
                }
            });
        });
    </script>
</body>
</html>
